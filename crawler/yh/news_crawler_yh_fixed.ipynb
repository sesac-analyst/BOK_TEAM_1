{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# br태그 오류 수정 코드\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from time import sleep\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 캐시를 위한 딕셔너리 초기화\n",
    "cached_data = {}\n",
    "\n",
    "# 날짜 범위를 1개월 단위로 나누는 함수\n",
    "def get_monthly_date_ranges(start_date, end_date):\n",
    "    date_ranges = []\n",
    "    current_start = start_date\n",
    "    while current_start < end_date:\n",
    "        current_end = min(current_start + relativedelta(months=1) - timedelta(days=1), end_date)\n",
    "        date_ranges.append((current_start, current_end))\n",
    "        current_start = current_end + timedelta(days=1)\n",
    "    return date_ranges\n",
    "\n",
    "# 파일명에 사용할 수 없는 문자 제거 함수\n",
    "def clean_filename(filename):\n",
    "    filename = re.sub(r'[\\/:*?\"<>|.]', '_', filename)\n",
    "    filename = re.sub(r'_+', '_', filename)\n",
    "    return filename.strip('_')\n",
    "\n",
    "# HTTP GET 요청을 재시도하는 함수\n",
    "def fetch_url_with_retries(url, headers, retries=10, timeout=10):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            # print(f\"{datetime.now()} - Attempt {i+1} to fetch URL: {url}\")\n",
    "            res = requests.get(url, headers=headers, timeout=timeout)\n",
    "            if res.status_code == 200:\n",
    "                return res\n",
    "            else:\n",
    "                print(f\"Unexpected status code {res.status_code} for URL: {url}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"{datetime.now()} - Request failed ({i+1}/{retries}): {e}\")\n",
    "            sleep(2)\n",
    "    print(f\"{datetime.now()} - Failed to retrieve JSON data from {url}\")\n",
    "    return None\n",
    "\n",
    "# HTML 콘텐츠를 정리하는 함수 (br 태그를 줄바꿈으로 변환)\n",
    "def clean_html_content(contents):\n",
    "    for br in contents.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "    return contents.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "# 기사 데이터 수집 및 저장 함수\n",
    "def process_article(contents_id, headers, directory):\n",
    "    base_url = f'https://www.yna.co.kr/view/{contents_id}?section=search'\n",
    "\n",
    "    # 캐시된 데이터 사용 시도\n",
    "    if contents_id in cached_data:\n",
    "        print(f\"Using cached data for article {base_url}\")\n",
    "        crawling_soup = cached_data[contents_id]\n",
    "    else:\n",
    "        crawling_res = fetch_url_with_retries(base_url, headers)\n",
    "        if crawling_res is None:\n",
    "            print(f\"Failed to retrieve the article {base_url}\")\n",
    "            return\n",
    "        crawling_soup = bs(crawling_res.text, 'html.parser')\n",
    "        cached_data[contents_id] = crawling_soup  # 캐싱\n",
    "\n",
    "    try:\n",
    "        news_title = crawling_soup.select_one('h1.tit').text.strip()\n",
    "        news_date = crawling_soup.select_one('.txt-copyright > span.date').text.strip()[:10].replace(\"/\", \".\")\n",
    "        # contents = crawling_soup.find(\"article\", class_=\"story-news\").find_all('p', text=True)\n",
    "        # news_cont = clean_html_content(contents) if contents else \"내용을 찾을 수 없습니다.\"\n",
    "        # contents = contents[:-2]\n",
    "        \n",
    "        # 모든 p 태그를 찾아 리스트로 저장\n",
    "        p_tags = crawling_soup.find(\"article\", class_=\"story-news\").find_all('p')\n",
    "        \n",
    "        # 뒤에서 두 번째까지 제거한 후, 각 p 태그의 텍스트를 추출하여 하나의 문자열로 결합\n",
    "        if len(p_tags) > 2:\n",
    "            p_tags = p_tags[:-1]\n",
    "        \n",
    "        contents = \" \".join([p.get_text(strip=True) for p in p_tags])\n",
    "        # contents = re.sub(r'[\"“”\\'\\'`]+', '\"', contents)\n",
    "        # contents = contents.replace('\"\"', '\"')  # 연속된 쌍따옴표\n",
    "        # contents = contents.replace(\"“\", '\"').replace(\"”\", '\"')  # 특수 문자 따옴표\n",
    "        # contents = contents.replace(\"''\", '\"')  # 연속된 홑따옴표\n",
    "        \n",
    "        safe_title = clean_filename(news_title)\n",
    "        csv_file_path = f'{directory}/{news_date}_{safe_title}.csv'\n",
    "        \n",
    "        with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(['날짜', '제목', '내용', 'url'])\n",
    "            # csvwriter.writerow([news_date, news_title, news_cont, base_url])\n",
    "            csvwriter.writerow([news_date, news_title, contents, base_url])\n",
    "        print(f\"Saved CSV: {csv_file_path}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"Failed to extract data for article {base_url}: {e}\")\n",
    "\n",
    "# 페이지 데이터를 가져와서 처리하는 함수\n",
    "def process_page(page_no, from_date, to_date, headers, directory):\n",
    "    url = f'http://ars.yna.co.kr/api/v2/search.asis?callback=Search.SearchPreCallback&query=%EA%B8%88%EB%A6%AC&page_no={page_no}&period=diy&from={from_date}&to={to_date}&ctype=A&page_size=10&channel=basic_kr'\n",
    "    \n",
    "    res = fetch_url_with_retries(url, headers)\n",
    "    if res is None:\n",
    "        print(f\"Failed to retrieve data for page {page_no} in range {from_date} ~ {to_date}\")\n",
    "        return\n",
    "    try:\n",
    "        json_str = re.search(r'Search\\.SearchPreCallback\\((.*)\\)', res.text).group(1)\n",
    "        datas = json.loads(json_str)\n",
    "    except (AttributeError, json.JSONDecodeError) as e:\n",
    "        print(f\"Failed to parse JSON for page {page_no}: {e}\")\n",
    "        return\n",
    "    contents_ids = [item['CONTENTS_ID'] for item in datas.get('KR_ARTICLE', {}).get('result', [])]\n",
    "    with ThreadPoolExecutor(max_workers=5) as article_executor:\n",
    "        futures = [article_executor.submit(process_article, contents_id, headers, directory) for contents_id in contents_ids]\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()  # Raise exceptions if any occurred\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article: {e}\")\n",
    "\n",
    "# 메인 함수\n",
    "def main():\n",
    "    start_date = datetime.strptime('20240101', '%Y%m%d')\n",
    "    end_date = datetime.strptime('20240811', '%Y%m%d')\n",
    "    date_ranges = get_monthly_date_ranges(start_date, end_date)  # 1개월 단위로 변경\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36',\n",
    "        'Referer': 'https://www.yna.co.kr/'\n",
    "    }\n",
    "    directory = './dataset'\n",
    "    os.makedirs(directory, exist_ok=True)  # Ensure dataset directory exists\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as page_executor:\n",
    "        for date_range in date_ranges:\n",
    "            from_date = date_range[0].strftime('%Y%m%d')\n",
    "            to_date = date_range[1].strftime('%Y%m%d')\n",
    "            futures = [page_executor.submit(process_page, page_no, from_date, to_date, headers, directory) for page_no in range(1, 51)]\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    future.result()  # Raise exceptions if any occurred\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing page: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
