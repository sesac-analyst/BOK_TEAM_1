{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429e6d4-082e-4d74-9ca7-1241107244c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#멀티스레드,타임슬림(랜덤 1~3초),header naver-> nate\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import winsound\n",
    "\n",
    "# Manually specify the start and end dates\n",
    "sc_sdate = '2015-01-01'  # Start date (YYYY-MM-DD)\n",
    "sc_edate = '2015-12-31'  # End date (YYYY-MM-DD)\n",
    "\n",
    "# Example URL using manual dates\n",
    "url = f\"https://news.einfomax.co.kr/news/articleList.html?page=1&total=6417&sc_section_code=&sc_sub_section_code=&sc_serial_code=&sc_area=A&sc_level=&sc_article_type=&sc_view_level=&sc_sdate={sc_sdate}&sc_edate={sc_edate}&sc_serial_number=&sc_word=%EA%B8%88%EB%A6%AC&box_idxno=&sc_multi_code=&sc_is_image=&sc_is_movie=&sc_user_name=&sc_order_by=E&view_type=sm\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',\n",
    "    'Referer': 'https://www.nate.com/'\n",
    "}\n",
    "base = 'https://news.einfomax.co.kr'\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "total = soup.select_one('#sections > section > header > h3 > small').text\n",
    "total_num = ''.join([i for i in total if i.isdigit()])\n",
    "total_num = int(total_num)\n",
    "pages = (total_num + 19) // 20  # Total pages\n",
    "\n",
    "# Ensure the directory exists\n",
    "output_dir = r\"C:\\Users\\rapha\\Desktop\\workspace\\study\\PJT\\crawling\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize a lock for thread-safe file operations\n",
    "file_lock = threading.Lock()\n",
    "\n",
    "# Function to crawl a single page\n",
    "def crawl_page(page):\n",
    "    try:\n",
    "        url = f\"https://news.einfomax.co.kr/news/articleList.html?page={page}&total=6417&sc_section_code=&sc_sub_section_code=&sc_serial_code=&sc_area=A&sc_level=&sc_article_type=&sc_view_level=&sc_sdate={sc_sdate}&sc_edate={sc_edate}&sc_serial_number=&sc_word=%EA%B8%88%EB%A6%AC&box_idxno=&sc_multi_code=&sc_is_image=&sc_is_movie=&sc_user_name=&sc_order_by=E&view_type=sm\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        li_tg = soup.select('ul.type2>li>h4.titles>a')  # 해당 페이지에 있는 뉴스기사 링크 리스트\n",
    "        for i in li_tg:\n",
    "            target = i.attrs['href']\n",
    "            crawling_url = base + target\n",
    "            response = requests.get(crawling_url, headers=headers)\n",
    "            crawling_soup = BeautifulSoup(response.text, 'html.parser')  # 해당 뉴스기사 링크의 html 정보 추출\n",
    "            title = crawling_soup.select_one('h3.heading').text\n",
    "            new_title = title.replace(\"/\", \"_\")  # 타이틀 전처리 결과\n",
    "\n",
    "            date_li = crawling_soup.select('ul.infomation>li')[1].text\n",
    "            date = date_li.split(\"입력\")[-1].replace('.', '_').replace(':', '_')\n",
    "\n",
    "            info = crawling_soup.select_one('#article-view-content-div').text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "\n",
    "            file_path = os.path.join(output_dir, f'{date}_연합인포맥스.text')\n",
    "            \n",
    "            with file_lock:\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write('제목\\n')\n",
    "                    f.write(f'{new_title}\\n')\n",
    "                    f.write('\\n')\n",
    "                    f.write('내용\\n')\n",
    "                    f.write(f'{info}\\n')\n",
    "\n",
    "            print(f'Saved file: {file_path}')\n",
    "            time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred on page {page}: {e}\")\n",
    "\n",
    "# Use ThreadPoolExecutor to manage threads\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    executor.map(crawl_page, range(1, pages + 1))\n",
    "\n",
    "\n",
    "# Play a sound when crawling is complete\n",
    "def play_sound():\n",
    "    winsound.Beep(440, 1000)  # Frequency 440Hz, Duration 1000ms (1 second)\n",
    "\n",
    "print(\"Crawling End\")\n",
    "play_sound()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ac1ef-04b5-4a7d-9b81-6bfa66cbc007",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
